,0
0,"
Bias and Debias in Recommender System: A Survey and Future
Directions
JIAWEI CHEN and HANDE DONG, University of Science and Technology of China, China
XIANG WANG and FULI FENG, National University of Singapore, Singapore
MENG WANG, Hefei University of Technology, China
XIANGNAN HE‚Ä†,University of Science and Technology of China, China
While recent years have witnessed a rapid growth of research papers on recommender system , most of the papers
focus on inventing machine learning models to better fit user behavior data. However, user behavior data is observational
rather than experimental. This makes various biases widely exist in the data, including but not limited to selection bias,
position bias, exposure bias, and popularity bias. Blindly fitting the data without considering the inherent biases will result in
many serious issues, e.g., the discrepancy between offline evaluation and online metrics, hurting user satisfaction and trust
on the recommendation service, etc. To transform the large volume of research models into practical improvements, it is
highly urgent to explore the impacts of the biases and perform debiasing when necessary. When reviewing the papers that
consider biases in RS, we find that, to our surprise, the studies are rather fragmented and lack a systematic organization.
"
1,"The
terminology ‚Äúbias‚Äù is widely used in the literature, but its definition is usually vague and even inconsistent across papers.
This motivates us to provide a systematic survey of existing work on RS biases. In this paper, we first summarize seven types
of biases in recommendation, along with their definitions and characteristics. We then provide a taxonomy to position and
organize the existing work on recommendation debiasing. Finally, we identify some open challenges and envision some future
directions, with the hope of inspiring more research work on this important yet less investigated topic.
"
2,"The summary of
debiasing methods reviewed in this survey can be found at https://github.com/jiawei-chen/RecDebiasing.
CCS Concepts: ‚Ä¢Information systems ‚ÜíRecommender systems .
Additional Key Words and Phrases: Sampling, Recommendation, Efficiency, Adaption
ACM Reference Format:
Jiawei Chen, Hande Dong, Xiang Wang, Fuli Feng, Meng Wang, and Xiangnan He‚Ä†. 2020. Bias and Debias in Recommender
System: A Survey and Future Directions.(December 2020), 38 pages. https://doi.org/10.1145/1122445.1122456
"
3,"1 INTRODUCTION
Being able to provide personalized suggestions to each user, recommender system has been recognized as the
most effective way to alleviate information overloading. It not only facilitates users seeking information, but also
benefits content providers with more potentials of making profits. Nowadays, recommendation techniques have
been intensively used in countless applications, E-commerce platforms (Alibaba, Amazon), social networks
‚Ä†Corresponding author: hexn@ustc.edu.cn.
Authors‚Äô addresses: Jiawei Chen, cjwustc@ustc.edu.cn; Hande Dong, donghd@mail.ustc.edu.cn, University of Science and Technology of
China, Hefei, China; Xiang Wang, xiangwang@u.nus.edu; Fuli Feng, fulifeng93@gmail.com, National University of Singapore, Singapore,
Singapore; Meng Wang, eric.mengwang@gmail.com, Hefei University of Technology, Hefei, China; Xiangnan He‚Ä†, hexn@ustc.edu.cn,
University of Science and Technology of China, Hefei, China.
"
4,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that
copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first
page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy
otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from
permissions@acm.org.
"
5,"(Facebook, Weibo), video-sharing platforms (YouTube, TikTok), lifestyle apps (Yelp, Meituan), and so on. As such,
the importance of RS cannot be overstated especially in the era that the information overload issue becomes
increasingly serious.
Ubiquity of Biases in RS. Although RS has generated large impacts in a wide range of applications, it faces
many bias problems which are challenging to handle and may deteriorate the recommendation effectiveness. Bias
is common in RS for the following factors. User behavior data, which lays the foundation for recommendation
model training, is observational rather than experimental. The main reason is that a user generates behaviors on
the basis of the exposed items, making the observational data confounded by the exposure mechanism of the
system and the self-selection of the user. Items are not evenly presented in the data. some items are more
popular than others and thus receive more user behaviors.
"
6,"As a result, these popular items would have a larger
impact on the model training, making the recommendations biased towards them. The same situation applies to
the user side. (3) One nature of RS is the feedback loop ‚Äî the exposure mechanism of the RS determines user
behaviors, which are circled back as the training data for the RS. Such feedback loop not only creates biases but
also intensifies biases over time, resulting in ‚Äúthe rich get richer‚Äù Matthew effect.
Increasing Importance of Biases in RS Research. Recent years have seen a surge of research effort on
recommendation biases. Figure 1 shows the number of related papers in top venues increases significantly since
the year of 2015. The prestigious international conference on information retrieval, SIGIR, has organized specific
sessions in 2020 and 2021 to discuss topics on bias elimination1. SIGIR even presents the Best Paper award to
the paper on this topic in 2018 respectively. The conferences Recsys and
WWW also organized tutorial on this topic in 2021.
"
7,"Biases not only draw increasing attention from the
information retrieval academia, but also from the industry. For example, one competing task of KDD Cup 2020
organized by Alibaba is to handle the long-tail bias in E-commerce recommendation.
Necessity of this Survey. Although many papers are published on this topic recently, to the best of our
knowledge, none of them has provided a global picture of the RS biases and corresponding debiasing techniques.
Particularly, we find that current studies on this topic are rather fragmented ‚Äî despite the wide usage of the
terminology ‚Äúbias‚Äù in the literature, its definition is usually vague and even inconsistent across papers. For
example, some work use ‚Äúselection bias‚Äù to denote the bias of observed rating values, while others use
‚Äúobservational bias‚Äù to refer to the same meaning instead. More confusingly, the same terminology ‚Äúselection
bias‚Äù has been conceptualized differently in different publications. Moreover, a considerable
number of researchers do not explicitly mention ‚Äúbias‚Äù or ‚Äúdebias‚Äù in the paper, but they
indeed address one type of biases in RS; these significant related work is difficult to be retrieved by the researchers
interested in the bias topic.
"
8,"Given the increasing attention of biases in RS, the rapid development of debiasing
techniques, and the flourishing but fragmented publications, we believe it is the right time to present a survey of
this area, so as to benefit the successive researchers and practitioners to understand current progress and further
work on this topic.
Difference with Existing Surveys. A number of surveys in recommendation have been published recently,
focusing on different perspectives of RS. For example, reviews explainable recommendation, reviews
knowledge-based recommendation, and summarize the recommendation methods based on deep
learning and reinforcement learning, respectively. However, to our knowledge, the perspective of bias has
not been reviewed in existing RS surveys. There are some surveys on the bias issues, but they are not on the
recommendation task. For example, recently reviews the bias issues in natural language processing,
reviews the sample selection bias on model estimation, summarizes fairness in learning-based sequential
decision algorithms. There are some surveys on the bias and fairness of general machine learning and artificial"
9,"1. The statistics of publications related to biases in RS with the publication year and venue.
intelligence systems. Comparing to the bias issues in other tasks, bias in RS has its own
characteristics, requiring a new inclusive review and summary. To this end, we make the following contributions
in this survey:
‚Ä¢Summarizing seven types of biases in RS and providing their definitions and characteristics. Wherein, we
specifically provide causality-based explanations for data biases to help the readers to better understand
their nature.
‚Ä¢Conducting a comprehensive review and providing a taxonomy of existing methods on recommendation
debiasing, as well as discussing their strengths and weaknesses.
‚Ä¢Identifying open challenges and discussing future directions to inspire more research on this topic.
Papers Collection. We collect over papers that analyze the bias issues in recommendation or propose new
debiasing methods. We first search the related top-tier conferences and journals to find related work, inculding
WWW, WSDM, SIGIR, KDD, RecSys, CIKM, TOIS, TKDE, etc., with the keywords ‚Äúrecommend‚Äù, ‚Äúcollaborative
filtering‚Äù, ‚Äúranking‚Äù or ‚Äúsearch‚Äù combined with ‚Äúbias‚Äù, ‚Äúfairness‚Äù or ‚Äúexposure‚Äù from the year 2010 to 2021. We
then traverse the citation graph of the identified papers, retaining the papers that focus bias in RS. Figure 1
illustrates the statistics of collected papers with the publication time and venue.
"
10,"Survey Audience and Organization. This survey is beneficial for the following researchers and practitioners
in RS: 1) who are new to the bias issues and look for a handbook to fast step into this area, 2) who are confused
by different bias definitions in the literature and need a systematic study to understand the biases, 3) who
want to keep up with the state-of-the-art debiasing technologies in RS, and 4) who face bias issues in building
recommender systems and look for suitable solutions. The rest of the survey is organized as follows: Section 2
introduces the preliminaries of RS and the critical issue of feedback loop. Section 3 and 4 are the main content,
which summarizes the seven types of biases and provides a taxonomy of debiasing technologies in RS. Section 5
discusses open challenges and future directions, and Section 6 concludes the survey.
2 PRELIMINARIES: RECOMMENDER SYSTEM AND FEEDBACK LOOP
"
11,"2.1 Feedback Loop in Recommendation
From a bird‚Äôs-eye view, we can abstract the lifecycle of recommendation as a feedback loop among three key
components: User, Data, and Model. As Figure 2 shows, the feedback loop consists of three stages. Publication date: December 2020.4 ‚Ä¢Chen and Dong, et al.
‚Ä¢User‚ÜíData (Collection), which indicates the phase of collecting data from users, including user-item
interactions and other side information ( e.g.,user profile, item attributes, and contexts).
‚Ä¢Data‚ÜíModel (Learning), which represents the learning of recommendation models based on the collected
data. At its core is to derive user preference from historical interactions, and predict how likely a user
would adopt a target item. Extensive studies have been conducted over past decades.
‚Ä¢Model‚ÜíUser (Serving), which returns the recommendation results to users, so as to satisfy the information
need of users. This stage will affect the future behaviors and decisions of users.
Through this loop, users and the RS are in a process of mutual dynamic evolution, where personal interests
and behaviors of users get updated via recommendation, and the RS can lead to a self-reinforcing pattern by
leveraging the updated data.
"
12,"2.2 Recommendation Task Formulation
We use uppercase character to denote a random variable; lowercase character to denote its specific
value; and the character in calligraphic font to represent the space of the corresponding random variable.
The probability distribution of a random variable is notated with, while the expectation of a function of a
random variable notated with .
Suppose we have a recommender system with a user set an item. We denote the collected
interactions the user set item setI, as a list of user-item-label triplets , which are
drawn from an unknown training distribution . Hereùëü‚ààR denotes the feedback label given by a
user to an item. It can be explicit that directly reflects user preference on the rated item, or
be implicit indicating whether the user willing to interact with the item. For
better description, we use the notation ùëüùë¢ùëñwith the subscript denoting the ground-truth label of the user-item
pair, while use the 
ùë¢ùëñdenoting the observed label of in training data. Besides, we define a Bernoulli
random variable indicating whether the instance is observed in use notation a certain
user-item pair. The task of a recommendation system can be stated as follows:
learning a recommendation model from the available dataset that it can capture user preference and make
a high-quality recommendation in the serving stage. Formally, let denote the error function measuring the
distance between the prediction and the ground truth. The goal of recommendation is to learn a parametric
model minimize the following True Risk :
where denotes the ideal unbiased data distribution for model testing. This distribution can be factorized
as the product of the user-item pair distribution and the factual preference
distribution for each user-item pair. As such, the True Risk is often written as a metric over all user-item
pairs:
loss of generalization, here we just present the point-wise loss. In fact, other types of losses can be extended straightforwardly if we
regard the item pair or list as an instance.
Publication date: December 2020.Bias and Debias in Recommender System: A Survey and Future Directions ‚Ä¢
User
Recommender
System 
Collection
Data
Serving
(clicks,¬†rates¬†‚Ä¶)¬†
 Learning
(Top‚ÄêN¬†recommendation) ¬†
User
Recommender
System 
Data
User-selection Bias
Exposure Bias
Conformity Bias
Position Bias
Popularity Bias
UnfairnessInductive Bias
Bias Amplification
along the LoopFeedback Loop. Feedback loop in recommendation, where biases occur in different stages. The characteristics of seven types of biases in recommendation and the bias amplification in loop.
"
13,"Types Stages in Loop Cause Effect Major solutions
Selection Bias User Data Users‚Äô self-selectionSkewed observed
rating distributionData Imputation; Propensity Score;
Joint Generative Model; Doubly Robust Model
Exposure Bias User DataItem Popularity;
Intervened by systems;
User behavior and backgroundUnobserved interactions
do not mean negativeGiving confidence weights by heuristic,
sampling or exposure-based model;
Propensity Score; Causality-based Model
Conformity Bias User Data Conformity Skewed interaction labels Modeling social or popularity effect
Position Bias User DataTrust top of lists;
Exposed to top of listsUnreliable positive
dataClick models; Propensity Score; Trust aware Model
Inductive Bias Data‚ÜíModel Added by researchers or engineersBetter generalization,
lower variance or
Faster recommendation
Popularity Bias Model User Algorithm and unbalanced data Matthew effectRegularization; Adversarial Learning;
Causal Graph
Unfairness Model User Algorithm and unbalanced data Unfairness for some groupsRebalancing; Regularization;
Adversarial Learning; Causal Modeling
Bias amplification
in LoopAll Feedback loop Enhance and spread biasBreak the loop by collecting random
data or using reinforcement learning
If the training data and test data are identically and independently distributed, the
empirical risk would be an unbiased estimator of the true risk. The PAC
learning theory states that the learned model will be approximately optimal if we have sufficiently large
training data set.
"
14,"3 BIAS IN RECOMMENDATION
In this section, we first summarize and position different types of biases in the feedback loop, as illustrated in
Figure  and Table . We then present in-depth analyses of their relations and discuss how they are intensified
along the feedback loop.
3.1 Bias in Data
As the data, of user interactions, are observational rather than experimental, biases are easily introduced into
the data. They typically stem from different subgroups of data, and make the recommendation models capture
these biases and even scale them, thereby leading to systemic racism and suboptimal decisions. In this subsection. Publication date: December‚Ä¢Chen and Dong, et al.
LDTest data 
distributionBiases Training data distribution
illustration of the data bias and its negative effect on model training.
we would first give the general definition of the data bias and then categorize it into four groups: selection bias,
conformity bias, exposure bias and position bias.
"
15,"3.1.1 Definition of data bias. The . assumption lays a foundation for recent learning-based methods to
generalize well on the test environment. However, this assumption may not hold in real recommender systems.
Typically, the data collection process in RS is observational rather than experimental. The sample selection or
user decision would inevitably be affected by many undesirable factors, such as the exposure mechanism of RS or
public opinions, making the training data distribution deviate from test distribution. Training data only gives a
skewed snapshot of user preference, making the recommendation model sink into sub-optimal result. We name
such notorious distribution deviation phenomenon as data bias:
‚Ä¢Data Bias. The distribution for which the training data is collected is different from the ideal test data
distribution.
illustrates the data bias and its negative effect. Bias distorts training distribution, causing the model
flow towards wrong direction. The red curve denotes the true risk function for testing, while the blue curve
denotes the expected empirical risk function for training. As the two risks are expected over different distribution,
they will behave rather differently even in their optimum. It means that even if a sufficiently
large training set is provided and the model arrives at empirical optimal point , there still exists a certain gap
the optimum and the empirical one . Blindly fitting a recommendation model without
considering the inherent data bias will result in inferior performance.
In the following, we will introduce four types of data biases, with providing their definitions and characteristics.
We also provide causality-based explanations for each bias to help the readers to better understand its nature.
"
16,"3.1.2 Selection Bias. Selection bias originates from users numerical ratings on items,
which is defined as:
‚Ä¢Selection Bias. Selection Bias happens as users are free to choose which items to rate, so that the observed
ratings are not a representative sample of all ratings. In other words, the rating data is often missing not at
random.
Prior study conducted by Marlin et al. offers compelling evidence to show the existence of selection bias
in the rating data. In particular, they conducted a user survey to gather the user ratings to some randomly-selected
items, as a comparison with that to conventional user-selected items.summarizes the comparison and
offers two findings: users tend to select and rate the items that they like; and users are more likely to rate
particularly bad or good items. These results suggest that selection bias is inherent in the observed data, since
they are missing not at random. The distribution of observed rating data is different from the distribution of all
ratings.
"
17,"Selection Bias Exposure Bias Conformity Bias Position Bias
Causal graphs of four types of data bias. feedback label; observation variable exposure; public opinion; item position.
Causality-based Explanation. Figure illustrates the generative process of the observed rating data. The
links ùëÖrepresent the causal effect of the features of the user ùëàand itemùêºon their interaction label for which the recommendation model aims to estimate. In observational study, the collected rating data is
not evenly presented and the variables would affected the observation of the instance. This mechanism makes the distribution of the observed rating data 
inconsistent with the ideal test distribution. The model that directly learned on observed
data would suffer.
"
18,"From another perspective, the causal graph shows two sources of association between the causes and the
outcome the desirable causal effect the collision path andùëÖthrough their common (conditioned on) effects . The analyses conditioned on would create
spurious association between. The model learned on observed data would capture skewed patterns.
3.1.3 Exposure Bias. Implicit feedback is widely used in recommendation, which reflects natural behaviors of
users, such as purchases, views, clicks. Distinct from explicit feedback that offers numerical ratings, implicit
feedback only provides partial signal of positive. As the knowledge about what the user dislikes is not available,Article . Publication date: December 2020.8 ‚Ä¢Chen and Dong, et al.
the learning must rely on unobserved interactions, mining the negative signal from them. Exposure bias happens
in such one-class data, which is defined as:
‚Ä¢Exposure Bias. Exposure bias happens as users are only exposed to a part of specific items so that unobserved
interactions do not always represent negative preference.
"
19,"In particular, an unobserved interaction between a user and an item can be attributed to two possible reasons:
the item does not match user interest; and the user is unaware of the item. Hence, ambiguity arises in the
interpretation of unobserved interactions. The inability to distinguish real negative interactions from the potentially-positive ones will result in severe biases. Previous studies
have investigated several dimensions of data exposure Exposure is affected by the policy of the previous
recommender systems, which controls what items to show. Hence, some recent works also name
such ‚Äúexposure bias‚Äù as ‚Äúprevious model bias‚Äù. As users may actively search and find the items of interest, the
selection of users is a factor of exposure , and makes highly relevant items more likely to be exposed.
Hence, in this scenario, ‚Äúexposure bias‚Äù is named as ‚Äúuser-selection bias‚Äù. The background of users is another
factor to expose items, such as social friends , communities that they belong to 
Popular items are more likely to be seen by users. Hence, such ‚Äúpopularity bias‚Äù is another form of ‚Äúexposure
bias‚Äù. In order to facilitate readers and prevent concept confusion, we use the unified standard definition,
‚Äúexposure bias‚Äù, throughout this paper, rather than the separated definitions of the aforementioned factors.
"
20,"Causality-based Explanation. Figure illustrates the generative process of the collected implicit feedback
data: the links the causal effect of user/item features on the feedback, for which the
recommender aims to estimate; the link ùëÖ ùëÜrepresents the missing mechanism in implicit feedback only
positive interactions are observed. As the negative instances are not available, the learning
must resort to the unobserved interactions. Recent work on implicit feedback would leverage the observation
variableùëÜas a surrogate label,marking observed interactions as positive while unobserved as negative. It is
rational in ideal scenario as the distribution is equal to. However, due to the wide existence of
the exposure bias in practical, the equation does not hold. It can be understood from the paths
where the user/item features would affect whether the item is exposed to the user; andùê∏
would further distort the distribution, as a user could only generate interactions on the exposed items
"
21,"From another perspective, Figure shows two sources of associations between the variables
the desirable causal effect along the the spurious association created by the exposure bias
through the paths.The correlations captured by the recommendation model may fail to reflect
the true preference.
3.1.4 Conformity Bias. Different from the aforementioned biases that contribute on the data observation,
conformity bias distorts user judgment, which is defined as follow:
‚Ä¢Conformity Bias. Conformity bias happens as users tend to behave similarly to the others in a group, even
if doing so goes against their own judgment, making the feedback do not always signify user true preference.
For example, influenced by high ratings of public comments on an item, one user is highly likely to change her
low rate, avoiding being too harsh. Such phenomenon of conformity is common and cause biases
in user ratings. As shown in Krishnan et al., user ratings follow different distributions when users
rate items before or after being exposed to the public opinions. Moreover, conformity bias might be caused by
social influence, where users tend to behave similarly with their friends. Hence, the observed
interactions are skewed and might not reflect users‚Äô real preference on items.
"
22,"Causality-based Explanation. As illustrated in Figure , conformity bias can be understood from the
additional causal path . Public opinion depends on item property , Article . Publication date: December 2020.Bias and Debias in Recommender System: A Survey and Future Directions ‚Ä¢
and impacts user judgment . This undesirable phenomenon would distort the conditional distribution,
making the training distribution deviate from reflecting user true preference.
From another perspective, conformity bias creates spurious association between andùëÖthrough the path. The model trained on observed data would easily capture spurious association, leading to poor
performance.
3.1.5 Position Bias. Position bias is very common in recommendation, particularly in advertisement system or
search engine:
‚Ä¢Position Bias. Position bias happens as users tend to interact with items in higher position of the recom-
mendation list regardless of the items‚Äô actual relevance so that the interacted items might not be highly
relevant.
"
23,"Here ‚Äúrelevance‚Äù is widely used in the field of information retrieval, which denotes how the items are preferred by
the users. Popularity bias happens in implicit feedback data and describes a tendency of users to notice or interact
with items in certain positions of lists with higher probability, regardless of the items‚Äô actual relevance.
For example, recent studies on eye tracking demonstrate that users are less likely to browse items that are
ranked lower in vertical lists, while they only examine the first few items at the top of lists. Moreover,
Maeve et al. shows that users often trust the first few results in the lists and then stop assessing the rest,
without evaluating the entire list holistically . As such, the data collected from user feedback towards the
recommended lists may fail to reflect user preference faithfully .
Causality-based Explanation. Figure illustrates the generative process of users‚Äô feedback on recom-
mendation lists, where each item is companied with a position in the previous recommendation list.
The effect of position outcome ùëÜis along the path , the display position would
impact the probability that the item is exposed to the us along the path , the position would
also hinder users‚Äô own judgement, as users trust the recommender system and may over-estimate the relevance
of the highly-ranked items. As such, position bias is quite complex and would skew both the data observation
as well as the user judgment , making the training distribution deviate significantly from
the ideal test one.
"
24,"From another perspective, position bias creates two spurious associations between the
paths, which should be conquered.
3.2 Bias in Model
Bias is not always harmful. In fact, a number of inductive biases have been added deliberately into the model
design to achieve some desirable characteristics:
‚Ä¢Inductive Bias. Inductive bias denotes the assumptions made by the model to better learn the target function
and to generalize beyond training data.
The ability to generalize the prediction to unseen examples is the core of machine learning. Without assumptions
on the data or model, generalization cannot be achieved since the unseen examples may have an arbitrary output
space. Similarly, building a RS needs to add some assumptions on the nature of the target function. For example,
Johnson et al. assumes an interaction can be estimated by embedding inner product, while He et al.
adopts the neural network as its better generalization. Besides target function, inductive bias have been added
in other aspects. An example is the adaptive negative sampler, which aims to over-sample
the ‚Äúdifficult‚Äù instances in order to increase learning speed, even though the resultant loss function will differ
significantly from the original. Another example is the discrete ranking model which embeds user
and items as binary codes to improve the efficiency of recommendation, which is at the expense of sacrificing the
representation ability.
"
25," Item popularity VS. recommendation frequency, where items were classified into
three different groups:  denoting the set of most popular items that take up around 20 entire ratings, denoting the set
of most unpopular items that take up 20% entire ratings and M denotes the rest. The figure was reproduced from with
authors‚Äô permission.
3.3 Bias and Unfairness in Results
Besides the aforementioned biases in data or model, two important biases in recommendation results have been
studied, which are defined as follows:
‚Ä¢Popularity Bias. Popular items are recommended even more frequently than their popularity would war-
rant .
"
26,"The long-tail phenomenon is common in RS data: in most cases, a small fraction of popular items account for
the most of user interactions. When trained on such long-tailed data, the model usually gives higher scores to
popular items than their ideal values while simply predicts unpopular items as negative. As a result, popular items
are recommended even more frequently than their original popularity exhibited in the dataset. Popularity bias
has been empirically verified by Abdollahpouri et al. Figure 6 shows relationship between item popularity
and recommendation frequency. We can find most of recommended items are located at high popularity area.
In fact, they are recommended to a much greater degree than even what their initial popularity warrants .
Ignoring the popularity bias results in many issues: 1) It decreases the level of personalization and hurts the
serendipity. Since the preferences of different users are diverse, always recommending popular items will hurt
user experience, especially for the users favoring niche items. It decreases the fairness of the recommendation
results . Popular items are not always of high quality. Over-recommending popular items will reduce the
visibility of other items even if they are good matches, which is unfair.Popular bias will further increase the
exposure opportunities of popular items, making popular items even more popular ‚Äì the collected data for future
training becomes more unbalanced, raising the so-called ‚ÄúMatthew effect‚Äù issue.
"
27,"Another type of bias arises in the recommendation results is unfairness. Fairness has attracted increasing
attention in recent years. A consensual definition of fairness is ‚Äú absence of any prejudice or favoritism towards an
individual or a group based on their intrinsic or acquired traits ‚Äù , and the unfairness can be defined as follow:
‚Ä¢Unfairness. The system systematically and unfairly discriminates against certain individuals or groups of
individuals in favor others .
Unfairness issue has been an obstacle to making recommender systems more entrenched within our society.
In particular, based on attributes like race, gender, age, education level, or wealth, different user groups are
usually unequally represented in data. When training on such unbalanced data, the models are highly likely to
learn these over-represented groups, reinforce them in the ranked results, and potentially result in systematic
discrimination and reduced the visibility for disadvantaged groups ( e.g.,under-representing the minorities, racial
or gender stereotypes). For example, in the context of job recommendation, previous work found
that, compared to men, women saw less ads about high paying jobs and career coaching services, which is
Article . Publication date: December 2020.Bias and Debias in Recommender System: A Survey and Future Directions .
Biases¬†in¬†data
Conformity ¬†Bias
Position¬†Bias
Selection¬†Bias
Exposure¬†BiasOn¬†user¬†judgment
On data¬†observation
imbalance
Popularity ¬†Bias
UnfairnessBiases¬†in¬†ResultsInductive ¬†BiasBias¬†in¬†Model
Via¬†serving¬†&¬†collection ¬†stage
Relations between seven types of biases.
caused by gender imbalance. Analogously, friend recommendation in social graphs may reinforce historical
biases towards a majority and prevent minorities from being social influencers with high reach . Another
similar issue has been found in book recommendation, where the methods prefer recommending books of male
authors. Analyzing the unfairness issue inherent in recommendation is therefore becoming essential and
desirable.
3.4 Feedback Loop Amplifies Biases
Real-world recommender systems usually create a pernicious feedback loop. Previous subsections summarize the
biases occurred in different stages of the loop, while these biases could be further intensified over time along the
loop. To better understand this effect, figure 7 illustrates the relations of seven types of biases. Data biases would
incur or intensify the data imbalance, exacerbating bias issues in recommendation results ( Data bias‚ÜíData
imbalance‚ÜíBias in Results ); while the biased recommendations would in turn impact the decisions, exposure,
and selections of users, reinforcing the biases in users‚Äô future behaviors ( Bias in Results‚ÜíData Bias ).
"
28,"Taking the
position bias as an example, top items typically benefit from a greater volume of traffic, which in turn increases
their ranking prominence and the volume of traffic they receive, resulting in a rich-get-richer scenario.
Many researchers also study the impact of feedback loop on the popularity bias. Their simulated
results show that feedback loop will amplify popularity bias, where popular items become even more popular
and non-popular items become even less popular. These amplified biases also will decrease the diversity and
intensify the homogenization of users, raising the so-called ‚Äúecho chambers‚Äù or ‚Äúfilter bubbles‚Äù.
Interestingly, figure also shows that some data biases can be self-reinforced ( Data bias Data imbalance
‚ÜíData bias ). Taking conformity bias as an example, as users tend to behave similarly to the major groups, the
extent of the data imbalance would increase with time going by, which in turn impacts and exacerbates the
conformity bias. Similar situations apply to the selection bias and exposure bias which are also affected by the
data imbalance.
"
29,"4 DEBIASING METHODS
A large number of methods have been proposed to mitigate the effects of bias or unfairness. Table 2 lists the
reviewed methods, as well as their strengths and weaknesses. we classify them according to which biases they
addressed and which types of methods they adopted.Article . Publication date: December 2020.12 ‚Ä¢Chen and Dong, et al.
A lookup table for the reviewed methods for recommendation debiasing.
Addressed issues Categories How? Strengths? Weaknesses? Publications
Selection BiasEvaluatorPropensity Score Weight the dataGeneral,
Theoretical-soundnessRequiring proper
propensities
ATOP Specific design Theoretical-soundnessRequiring two
strong assumptions
TrainingJoint Generative Model Model missing mechanism ExplainableRequiring assumptions
on data generation,
Hard to train
Data Imputation Impute pseudo-labels SimpleHighly sensitive
to pesduo-labels
Propensity Score Weight the data Theoretical-soundnessHigh variance,
Requiring proper
propensities
Doubly Robust Model Impute+WeightTheoretical-soundness,
RobustRequiring proper
propensities
or pesduo-labele
Conformity BiasModeling popularity influenceDisentangle conformity
effect from user preferenceExplainableRequiring assumptions
on data generation
Modeling social influenceDisentangle social effect
from user preferenceExplainableRequiring assumptions
on data generation
Exposure BiasEvaluator Propensity Score Weight the data Theoretical-soundnessRequiring proper
propensities
TrainingHeuristicDown-weight unobserved
data heursticallySimpleCoarse-grained,
Heuristical
SamplingDown-weight unobserved
data via samplingEfficientCoarse-grained,
Requiring heuristic
or side information
Exposure-based modelWeight the data via
exposure modelExplainable,
Learn flexible weightsHard to train
Propensity Scores Weight the observed data Theoretical-soundnessRequiring proper
propensities,
High variance
Causality-based MethodsRemove spurious associations
via causal inferenceExplainableRequiring assumptions
on data generation
Others 
Position BiasClick ModelsModel the generative
process of clicksExplainableRequiring assumptions
on data generation,
hard to train
Propensity Score Weight the data Theoretical-soundnessHigh variance,
Requiring proper
propensities,
Fail to model trust
Trust-aware ModelsIntroduce offset terms
to remove trust effectTheoretical-soundness,
Capture trust effectHigh variance,
Requiring proper
propensities,
Fail to model trust
For multiple data biases
and their combinationsUniversal modelTransfer the knowledge
from unbiased data to
perform debiasingUniversal,
AdaptiveRequiring a set of
unbiased data
Popularity BiasRegularizationIntroduce regularization
termsSimple, Straightforward Possibly hurt accuracy
Adversarial LearningLeverage adversary to
bridge the gap between
niche and popular itemsBalancing representation Possibly hurt accuracy
Causal GraphLeverage causal graph
to elucidate and mitigate
popularity biasExplainableRequiring assumptions
on data generation
Others
UnfairnessRebalancingDirectly balance the data
or recommendation resultsStraightforward Possibly hurt accuracy
RegularizationFormulate the fairness
criteria as a regularizerStraightforward Possibly hurt accuracy
Adversarial LearningLeverage adversary to
isolate the effect of
sensitive attributesFair representation Possibly hurt accuracy
Causal ModelingEstimate fairness with
intervening sensitive
attributesExplainable,
Counterfactual fairnessRequiring assumptions
on data generation
Others
Loop effectUniform DataIntervene in the system
with a random logging policyStraightforward,
EffectiveHurting the user
experience and the
system profit
Reinforcement learningIntervene in the system
with a smarter strategy
for long-term benefitsAdaptively balancing
exploration-exploitationHard to train,
Off-policy evaluation
is chanllenging
Publication date: December 2020.Bias and Debias in Recommender System: A Survey and Future Directions ‚Ä¢
"
30,"4.1 Methods for Selection Bias
Training and testing a recommendation model on the observed rating data will suffer from the selection bias,
as the observed ratings are not a representative sample of all ratings. Here we fist introduce how to evaluate a
recommendation model under biased rating data, and then review four kinds of methods that mitigates selection
bias on recommender training.
4.1.1 Debiasing in evaluation. Given a recommendation model, we want to evaluate its performance on rating
prediction or recommendation accuracy. Standard evaluation metrics like Mean Absolute Error (MAE), Mean
Squared Error, Discounted Cumulative or Precision can be written as:
"
31,"where denotes indicator function the internal condition holds), ùëüùë¢ùëñdenotes the true rating values
of the itemùëñgiven by the user the predicted rating values by the recommendation model. As
true ratings ùëüare usually partially observed4. The conventional evaluation usually use the average over only the
observed entries:
"
32,"whereùë†ùë¢ùëñdenotes the number of observed ratings in the dataset. We can find is not an unbiased estimate
of the true performance :
"
33,"where is expected over the observation probability. The gap is caused by selection bias, making the
observed ratings not a representative sample of all ratings. Two strategies have been presented in recent work.
Propensity Score. To remedy the selection bias in evaluation, some recent work considers a recommen-
dation as an intervention analogous to treating a patient with a specific drug. In both tasks, we have only partial
knowledge of how much certain patients (users) benefit from certain treatments (items), while the outcomes for
most patient-treatment (user-item) pairs are unobserved. A promising strategy for both tasks is weighting the
observations with inverse propensity scores. The propensity, which is defined as the marginal probability
of observing a rating value for certain user-item pair can offset the selection bias. The
proposed estimator is defined as:
"
34,"Steck propose another unbiased metric ATOP to evaluate recommendation performance
with two mild assumptions:  the relevant (high) rating values are missing at random in the observed data;
Concerning other rating values, we allow for an arbitrary missing data mechanism, as long as they are missing
with a higher probability than the relevant rating values. They define the ATOP as:
"
35,"which computed from biased explicit feedback data and
ùë¢ denotes the number of observed relevant (preferred)
items of the user
ùë¢ counts the relevant ones in the top ùëò. The authors prove ATOPobs
ùë¢is an unbiased
estimate of the average recall and proportional to the precision averaged over users.
Discussion. Propensity scores and ATOP are two subtle strategies to remedy selection bias, but they still
have two serve weaknesses. The unbiasedness of the IPS-based estimator is guaranteed only when the true
propensities are available. The IPS estimator will still be biased if the propensities are specified unproperly.
"
36,"The unbiasedness of the ATOP is guaranteed only when the two assumptions hold. In practice, the missing
mechanism is often complex and the assumptions are not always valid. Developing a robust and effective remains
a challenge.
4.1.2 Debiasing in model training. In the following, we will review four kinds of methods on mitigating selection
bias on recommender training.
Joint Generative Model. Note that the main reason for the selection bias is that users are free to deliberately
choose which items to rate. Thus, a straightforward strategy for mitigating selection bias is to jointly consider both
rating prediction task and missing data prediction task. Some recent work propose to jointly model the generative
process of rating values and the missing mechanism. Their generative process can be depicted with Figure ,
with the assumption that the probability of users‚Äô selection on items depends on users‚Äô rating values
for that item . Correspondingly, ùë†ùë¢ùëñhas been modeled dependent on ùëüùë¢ùëñwith a mixture of Multinomials,
Logit model, MF model, binomial mixture model , or social-enhanced model. In this
way, user‚Äôs preference can not only learn from rating values but also from the missing mechanism.
"
37,"Although this kinds of methods is explainable and sometimes effective in some scenarios, jointly modeling the
missing mechanism and rating values will lead to a highly complex model, which is hard to be trained. What‚Äôs
worse, the architecture of missing data models are usually heuristically designed. The hypothesis on distribution
may not hold in some real cases.
Data Imputation. Note that the inherent nature of selection bias is that the data is missing not random.
A straightforward solution for selection bias is to impute the missing entries with pseudo-labels, such that
the observed data distribution is close to the ideal uniform one. For example, Steck et. Publication date: December 2020.Bias and Debias in Recommender System: A Survey and Future Directions ‚Ä¢
al. propose a light imputation strategy that directly impute the missing data with a specific value ,
with optimizing the following objective function:
"
38,"ùëäùë¢ùëñis introduced to
downweight the contribution of the missing ratings.
However, as imputed rating values are specified in a heuristic manner, this kind of methods will suffer
from empirical inaccuracy due to inaccurate imputed rating values. Such inaccuracy will be propagated into
recommendation model training, resulting in sub-optimal recommendation performance.
To resolve this issue, Saito et al. propose to learn imputation values with an asymmetric tri-training
framework. They first pre-train two predictors with two specific recommendation models to generate a
reliable dataset with pseudo-ratings and then trained a target recommendation model on the pseudo-ratings.
Theoretical analysis presented in shows that the proposed method optimizes the upper bound of the
ideal loss function. However, the performance of asymmetric tri-training depends on the quality of pre-trained
predictor , while a satisfied itself is hard obtained from biased data. Nevertheless, model-based imputation
strategy is a promising direction for mitigating selection bias, which deserve future exploring.
Propensity Score. Besides on evaluation, propensity score can be utilized to mitigate selection bias on model
training . This kind of methods directly use the IPS-based unbiased estimator as the objective and
optimize the following risk function:
"
39,"Except for the propensities that act like weights for each loss term, the training objective is
identical to the standard recommendation objective. Also, thanks to the propensities, the selection bias can be
mitigated as the IPS-based estimator is an unbiased estimation of the True Risk:
"
40,"However, as discussed in the previous subsection, specifying appropriate propensity scores is critical. The
performance of IPS-based model depends on the accuracy of the propensities. Moreover, propensity-based methods
usually suffer from high variance, leading to non-optimal results especially when the item popularity or
user activeness is highly skewed.
Doubly Robust Model. As data imputation-based models often have a large bias due to mis-specification
while IPS-based model usually suffer from high variance, Wang et al. propose to combine the two kinds of
models and enjoy a desired double robustness property: the capability to remain unbiased if either the imputed
errors or propensities are accurate. They define the following objective function:
"
41,"whereùëü
ùë¢ùëñdenotes the imputed value for certain user-item pair . The theoretical and empirical analyses
presented in validate superiority over both IPS-based and imputation-based models.
Although the model is more robust than single method, it still requires relatively accurate propensity score or
imputation data, which is usually hard to specify. Otherwise, its performance also suffers.Article . Publication date: December 2020.16 ‚Ä¢Chen and Dong, et al.
"
42,"4.2 Methods for Conformity Bias
Conformity bias occurs as users are normally influenced by others opinion so that the rating values are deviated
from users‚Äô true preference. Two types of methods have been proposed to address the conformity bias. The first
type of work considers users‚Äô behaviors conform to public opinions. For example, Liu et al. directly leverage
three important features the base recommendation model, where the number of ratings for
itemùëñbefore user ùë¢rates it, the average rating and the rating distribution. The predicted rating is
generated from XGBoost :
whereùë°ùë¢ùëñdenotes the prediction returned by basic recommendation model and the strength of
conformity. This way, we can disentangle the effect caused by conformity bias from users‚Äô true preference and
make a recommendation accordingly. Some recent work further study conformity bias in a more fine-grained
manner. Zheng et al. propose to model personalized conformity effect as users have different sensitivities to
the public opinions; while Zhao et al. model time-aware conformity effect by considering item dynamic
popularity.
The other type of methods treat user‚Äôs rating values as synthetic results of user preference and social influence. Thus, similar to, they directly leverage social factors in the base recommendation model
to generate final prediction and introduce specific parameters to control the effect of social conformity bias.
4.3 Methods for Exposure Bias
Exposure bias occurs as users are only exposed to a part of items so that unobserved interactive data does not
always mean negative signal. Exposure bias will mislead both the model training and evaluation. Here we review
the work on correcting exposure bias.
"
43,"4.3.1 Debiasing in evaluation . A straightforward strategy for debiasing in RS evaluation is using the inverse
propersity score, which also has been applied to address the selection bias. Yang et al. first illustrate
evaluation bias in terms of conventional metrics such on the implicit feedback data,
and leverage the IPS framework to offset the exposure bias.
"
44,"where the predicted ranking of item by the recommendation model and Gùë¢denotes
the set of all relevant items for user. needs to be adapted for different metrics, such as:
"
45,"However, due to the exposure bias, only partial preferred items are observed, making the model often be evaluated
on the biased implicit feedback as:
"
46,"ùë¢denotes the preferred items that have been exposed to the user . As users usually have biased exposure,
the output of evaluator does not conform the true performance.
To address this problem, similar to the treatment for selection bias in explicit feedback data, Yang et al.
propose to weight the each observation with the inverse of its propensity for implicit feedback data. The intuition
is to down-weight the commonly observed interactions, while up-weighting the rare ones. Thus, the IPS-based
unbiased evaluator is defined as follow:
"
47,"4.3.2 Debiasing in model training. To deal with the exposure bias and extract negative signal from the implicit
feedback, a conventional strategy is treating all the unobserved interactions as negative and specify their
confidence. The objective function of most such methods can be summarized as follow:
"
48,"a surrogate label indicating whether the interaction between user observed or not;
denotes the confidence weight, controlling the confidence that the the feedback of user-item pair should
be predicted as. The specification of the confidence weight is critical to the model performance and can be
roughly categorized into three types:
Heuristic Weighting. The first is heuristic-based strategy. For example, the classic weighted factorization
matrix and dynamic used a simple heuristic that the un-observed interactions are assigned
with a uniform lower weight. The intuition behind this
strategy is that unobserved data is relatively unreliable, which can be attributed to dislike or unknown; Some
researchers specify the confidence with based on user activity, as users
associate with more items provide more reliable information; Analogously, item popularity has been considered
to specify confidence weights , as popular items are more probable to be exposed; Also, user-item feature
similarity has been considered to define the confidence.
However, assigning appropriate confidence weights heuristically is challenging, as the optimal data confidence
may change for different user-item combinations. Choosing confidence weights usually require rich human
expertise or large computational resource for grid search. Furthermore, it is unrealistic to manually set flexible and
diverse weights for millions of data. Coarse-grained confidence weights will create empirical bias on estimating
user‚Äôs preference.
"
49,"Publication date: December 2020.18 ‚Ä¢Chen and Dong, et al.
Sampling. Another solution to address exposure bias is performing sampling. The sampling strategy deter-
mines which data are used to update parameters and how often, and thus scale the data contribution. Provided
the sampled probability of an instance is, learning a recommendation model with sampling is equivalent to
learning the model with the following weighted objective function:
where the sampling distribution acts as data confidence weights. Sampling strategy has been widely applied as its
efficiency. For example, Logistical matrix factorization, or most of neural-based recommendation
models apply the uniform negative sampler. considers
to over-sample the popular negative items, as they are more likely to be exposed. However, these heuristic
samplers are insufficient to capture real negative instances. Thus, some researchers explore to leverage side
information to enhance the sampler. Ding et al. leverage viewed but non-clicked data to evaluate user‚Äôs
exposure; Chen et al.leverage social network information in their sampling distribution; Wang et al.
construct an item-based knowledge graph and perform sampling on the graph.
"
50,"Another strategy is to develop an exposure-based model, which is capable of captur-
ing how likely a user is exposed to an item. EXMF introduces an exposure variable and assumes
the following generative process of implicit feedback:
whether a user been exposed to the item denotes delta function 
and can be relaxed as Bernoulli distribution parameterized with a small value; the prior probability of
exposure. When , we, since when the user does not know the item he can not interact with it.
the user has known the item, he will decide whether or not to choose the item based on his
preference. be generated with normal recommendation model. In this way, by optimizing the marginal
probability, the model can adaptively learn the exposure probability, which will be transformed as confidence
weights to remedy exposure bias.give detailed analyses of and rewrite the objective
function of EXMF as follows:"
51,"whereùõæùë¢ùëñis defined as variational parameters of the user‚Äôs exposure. is function:
where. We can find, which indicates how likely a user is exposed to an
item, acts as confidence weights to control the contribution of the data on learning a recommendation model.
This finding is consistent with our intuition. Only if the user has been exposed to the item, can he decide whether
or not to consume the items based on his preference. Thus, the data with larger exposure are more reliable in
deriving user preference.
However, directly estimating data confidence from Equation is insufficient as the model will easily suffer
from over-fitting and inefficiency problems due to the large scale of the inferred parameters . A promising
solution is to re-parameterize the confidence weights with a simpler function. For example, some researchers
propose to infer confidence wights with a social-based or community-based model.Article . Publication date: December 2020.Bias and Debias in Recommender System: A Survey and Future Directions ‚Ä¢
"
52,"Propensity Score. Although the aforementioned weighting strategies are popular and have been studied for
a long time, Saito et al. argue that these methods can not address exposure bias entirely ‚Äî For any choice
of the weights , The weighted empirical risk not be an unbiased estimator of the ideal True. To tackle this problem, Saito et al.proposes a new estimator with propensity score as
follow:
"
53,"where the propensity score is defined as the marginal probability of a user exposed to the item.an unbiased estimator of the True Risk :
"
54,"This propensity-based strategy is theoretical soundness and usually achieves better performance than weighting
strategies. It is flexible and also has been extended to pair-wise objective function. However, this
kind of methods has some limitations:its performance depends on the accuracy of the propensity score, which
is quite challenging to obtain; the inverse of propensity incurs high variance. Although these problems can
be mitigated to a certain extent by some strategies, clapping), they still deserve
further exploration.
"
55,"Causality-based Methods. Causal inference is another promising direction for addressing exposure bias.
In fact, the spirit of a recommendation can be understood as to answer a counterfactual question: would
the user interacts with the item if he had know the item? That is, we need to evaluate the causal estimand
with intervening item exposure5rather than the statistical associations estimated
by vanilla recommender models. The intervention could remove the spurious association caused by exposure
bias and recover users‚Äô true preference on the items.
Towards this target, various causality-based methods have been proposed. For example, Zhang et al.
resorted to the back-door criterion to remove the exposure bias caused by the item popularity; Xu et al.
leveraged forward door criterion to remove the effect from unobserved confounders; Wang et al.
leveraged counterfactual reasoning to eliminate the direct causal effect from exposure features on the prediction;
Yang et al.mitigates exposure bias through counterfactual samples; disentangle the effect
from exposure and preference with introducing information bottleneck.
"
56,"Others. There are also some other strategies to address exposure bias in specific scenarios. Wang et al.
consider the queries of a search system are usually under-sampled to different extents, and thus are biased when
click data is collected to learn the ranking function. They further propose a specific model for this situation, where
queries are classified into different classes, and the bias in each class is estimated with randomized data. Ovaisi et
al. attribute exposure bias to the fact that a user can examine only a truncated list of top-K recommended
items. To address this kind of exposure bias, two-step Hechman method has been adopted. They first use a Probit
model to estimate the probability of a document being observed and then leverage the exposure probability to
correct the click model. Some recent work also consider users‚Äô sequential behaviors ‚Äúexposure-click-conversion‚Äù
5Here the do-calculus .indicates the variable ùëÜis coercively intervened with a certain value. For more details on causal inference, we
refer the readers to the excellent causal textbook .Publication date: December ‚Ä¢Chen and Dong, et al.
and correspondingly devise an inverse propensity model , decomposition model or graph neural
network on the sequential behaviors to address exposure bias with multi-task learning. Besides, propensity
scoring model has been utilized in debiasing explainable recommendation and item-to-item recommendation.
"
57,"4.4 Methods for Position Bias
Position bias is another type of bias that is widely studied in learning-to-rank systems, such as ad system and
search engine. Position bias denotes that the higher ranked items will be more likely to be selected regardless of
the relevance. Recent years have seen a number of work on position bias, and we categorize them into three lines.
Click Models. The first line is based on click models. The methods make hypotheses about user browsing
behaviors and estimate true relevance feedback by optimizing the likelihood of the observed clicks. some work
on click models assume the examination hypothesis that if a displayed item is clicked, it
must be both examined and relevant. This is based on the eye-tracking studies which testify that users are less
likely to click items in lower ranks. To remedy position bias and to recover user true preference, they explicitly
model the probability of an user clicks an item ùëñat positionùëûas follows:
"
58,"Notice that a hidden random variable ùê∏has been applied, which denotes whether the user has examined the item.
In general, these methods make the following assumptions: if the user clicks it, the item must have been examined;
if the user has examined the item, the click probability only depends on the relevance; and the examination
depends solely on the position . The model is highly similar to the exposure-based model for exposure bias
except that the exposure probability is modeled with position.
Another choice of click model is the cascade model. It differs from the above model in that it aggregates
the clicks and skips in a single query session into a single model. It assumes a user examines an item from the
first one to the last one, and the click depends on the relevance of all the items shown above. Let the
probabilistic events indicating whether the item is examined and clicked respectively. The cascade model
generates users click data as follows:
"
59,"in which the Equation (40) implies that if a user finds her desired item, she immediately closes the session;
otherwise she always continues the examination. The cascade model assumes that there is no more than one click
in each query session, and if examined, an item is clicked with probability ùëüùë¢ùëû,ùëñand skipped with 1‚àíùëüùë¢ùëû,ùëñ. This
basic cascade model has been further improved by considering the personalized transition probability [ 26,59,224].
Jinet al. [74] improve these models and consider users browsing behaviors in a more thorough manner with
deep recurrent survival model.
However, these click models usually require a large quantity of clicks for each query-item or user-item pair,
making them difficult to be applied in systems where click data is highly sparse, e.g., personal search [ 169].
Further, mis-specifying the generative process of users clicks will cause empirical bias and hurt recommendation
performance.
, Vol. 1, No. 1, Article . Publication date: December 2020.Bias and Debias in Recommender System: A Survey and Future Directions ‚Ä¢21
Propensity Score. Another common solution to correct position bias is employing inverse propensity score,
where each instance is weighted with a position-aware values [6]. The loss function is defined as follow:
"
60,"Here we refer to and use the ranking metrics. denotes the metric function that is based on the the
rank of the item the user (or query) ùë¢according to the ranking system . For instance, it can be chosen to
match the well-known NDCG metric:
"
61,"A position-dependent propensity is introduced to weight the function. The intuition behind the model is
that clicks on items that are less likely to have been examined by users are weighted more heavily. This weighting
strategy compensates for the effect of position bias on user exposure, allowing the method to estimate and learn
without being affected by position bias in expectation Joachims et al.
Estimating the propensity score for position bias have been well explored as its simplicity just dependent
on the item position. A simple yet effective solution to estimate a position-based propensity model is result
randomization, where the ranking results are shuffled randomly and collect user clicks on different positions
to compute propensities scores. Because the expected item relevance is the same on all
positions, it is provable that the difference of click rate on different positions produces an unbiased estimation of
the truth propensities. Despite its simplicity and effectiveness, result randomization has a risk of significantly
hurting the user experience as the highly ranking items may not be favored by the user. Pair-wise swapping
has been proposed to mitigate the problem, but can not eliminate negative effect completely. Therefore, the
strategies that learn the propensity scores from the data without any intervention on the recommendation results
have been explored. Fang et al. and Agarwal et al. adopt intervention harvesting, to learn the propensity.
However, such methods require the feedback data from multiple ranking models. Further, some recent work
consider learning a propensity model and a recommendation model as dual problem and develop
specific EM algorithms to learn both models. More recently, the click model that captures the row skipping and
slower decay phenomenon has been adopted to specify the propensity scores in , while cascade model has
been adopted by . Chen also et al.
"
62,"propose to learn the propensity from the data observation.
Trust-aware Models. Item position not only influences users‚Äô exposure but also their decisions ( i.e.,Both
ùëÖandùê∏are dependent on ùëÑ). Aforementioned propensity score is insufficient to address this problem. Hence,
Agarwal et al. [7] propose an expansion to IPS to correct for both effects. The model hypothesizes that a real
relevant item at position ùëûcan be misjudged with probability 1‚àíùúñ+
ùëû, while a non-relevant item can be clicked
mistakenly with probability ùúñ‚àí
ùëû,i.e.,we have:
"
63,"further proof that insufficient
and propose a more theoretical-soundness method with affinity corrections:
"
64,"Publication date: December .
which is an unbiased estimation of the ideal estimator. position bias.
4.5 Universal Solutions for Various Data Biases
Most existing methods are designed for addressing one or two biases of a specific scenario. Hence, when facing
the real data that commonly contain multiple types of biases, these methods will fall short. Recently saw a few
studies on universal solutions for multiple data biases and their combinations. These methods resorted to a small
unbiased dataset for recommendation debiasing. For example, some work transfered the knowledge from the
unbiased data to the target model with domain adaption or knowledge distillation ; More recently,
Chen et al. proposed to learn the optimal debiasing configures from the uniform data with meta learning.
Despite their effectiveness on handle various data biases, these methods require unbiased data, which is difficult
and expensive to obtain. To collect uniform data, we must intervene in the system by using a random logging
policy instead of the normal recommendation policy, which would hurt users‚Äô experience and revenues of the
platform. Therefore, how to develop a universal solution without using unbiased data is still an open problem
and deserves further exploration.
"
65,"4.6 Methods for Popularity Bias
Popularity bias is a common problem in recommendation systems. We categorize the methods into four types.
Regularization. Suitable regularization can push the model towards balanced recommendation lists.
dollahpouri et al.introduced, the item embedding
matrix¬∑denotes the trace of a matrix, and the Laplacian matrix of item to the same set (popular items or long-tail items) and 0otherwise. Kamishima et al. applied
themean-match regularizer in their information-neutral recommender systems to correct popularity
bias. They first introduced mutual information to measure the influence of features on the recommendation
results, and through a series of mathematical approximations and derivations, they obtain a specific regularization
term:
"
66,"More recently, et al.
utilized a Pearson Coefficient regularizer to decrease the correlation between item popularity and model output
scores. Note that the above regularizers are result-oriented, guiding the model to give more balanced results.
Different from result-oriented regularizers, Chen et al.devise a process-oriented regularization term. It
attributes the inability of effectively recommending long-tail items as the insufficient training of them. These
items usually have few interaction records and thus their embedding vectors can not be well trained, making
their prediction scores close to the initial values and remain neutral. Motivated by this point, Chen et al. proposed
Entire Space Adaptation Model from the perspective of domain adaptation . ESAM aims to transfer
the knowledge from these well-trained popular items to the long-tail items. ESAM introduced three regularization
terms for transferring as:
"
67,"Domain adaptation with item embedding (or attributes) correlation alignment:
"
68,"ùê∑ùê∂: encouraging the
features of the items with the same feedback to be close together, and the features
of the items with different feedbacks to move away from each other. Self-training for target clustering ùêøùëù
ùê∑ùê∂:
minimizing the entropy regularization  favors a low-density separation between classes. This term is a
way of self-training which increases the discriminative power between non-displayed items.
Adversarial Learning. Adversarial learning is another line to address popularity bias et al. The basic idea
is to play a min-max game between the recommender G and the introduced adversary D, so that D gives a signal
to improve the recommendation opportunity of the niche items.
"
69,"Bias and Debias in Recommender System: A Survey and Future Directions ‚Ä¢
generated popular-niche item pairs , and an equal number of true popular-niche pairs as input.
True pairs are sampled from their global co-occurrence and synthetic pairs are drawn by the
recommender. The recommender G can be instantiated with recent recommendation model such as NCF. Through
adversarial learning between G and D, D learns the implicit association between popular and niche items, while
G learns to capture more niche items that correlate with the user‚Äôs history, resulting in recommending more
long-tail items for users.
"
70,"Causal Graphs. Causal graph is a powerful tool for counterfactual reasoning. Some recent work proposed
to leverage causal graph to tackle popularity bias. They first built a causal graph to elucidate popularity bias,
and then applied counterfactual intervention over the graph to mitigate the bias. For example, Zhang 
al.attributed the popularity bias to the undesirable causal effect from item popularity to the item exposure. To
eliminate this effect, they further proposed to intervene the distribution of the exposed items with back-door
criterion or propensity score; Zhao et al. and Wang et al. traced popularity bias from conformity effect
(i.e.,the effect of item popularity on user behavior), and causally intervened the item popularity to make fair
recommendation; Analogically, Wei et al.performed counterfactual reasoning to eliminate the direct effect
of item (popularity) to the prediction; Wang et al. studied how popularity bias occurs in model training.
They attributed the popularity bias to a confounding causal structure and applied backdoor adjustment to mitigate
this effect.
"
71,"Others. There are some other methods on popularity bias. one solution to reduce popularity bias is through
introducing other side information. For example, Bressan et al. leverage social information to reduce popularity
bias . Abdollahpouri gives a different strategy , which relies on re-ranking. To perform top-k recommen-
dation, it first generates a relatively large recommendation list with a classical model, and then re-ranks the
list by considering the item popularity. Similar to exposure bias, propensity score can also be applied to reduce
popularity bias: by decreasing the influence of popularity items to model training, the popularity bias can be
mitigated .
4.7 Methods for Unfairness
Before introducing existing fairness-aware methods, we first give some formulations of fairness.
4.7.1 Fairness Formulations. There are extensive studies on fairness in machine learning. Without loss of
generality, we use the notation of prediction model throughout this section to discuss fairness. Let the
set of sensitive attributes, other observed attributes, and unobserved attributes of an
individual, respectively. the ground-truth outcome to be predicted, while the prediction produced
by a prediction model that depends on. For simplicity we often assume is encoded as a binary attribute,
but this can be generalized to other cases.
There exist many different variations of fairness definition, which can be roughly categorized into four types: 1)
fairness through unawareness individual fairness group fairness
graphic parity, equality of opportunity , predictive equality , equalized odds, calibration
within groups counterfactual fairness . Here we present some widely-used formulations:
‚Ä¢Fairness Through Unawareness :A model is fair if any sensitive attributes ùê¥are not explicitly used in the
modeling process.
"
72,"‚Ä¢Individual Fairness :A model is fair if it gives similar predictions to similar individuals. Formally, if individu-
alsùëñandùëóare similar under a certain metric, their predictions should be similar: ÀÜùëå(ùëã(ùëñ),ùê¥(ùëñ))‚âà ÀÜùëå(ùëã(ùëó),ùê¥(ùëó)).
‚Ä¢Demographic Parity :Each protected group ( i.e.,with the same sensitive attributes) should receive positive
prediction at an equal rate. Formally, the prediction ÀÜùëåsatisfies demographic parity if ùëÉ(ÀÜùëå|ùê¥=0)=ùëÉ(ÀÜùëå|ùê¥=1).
, Vol. 1, No. 1, Article . Publication date: December 2020.24 ‚Ä¢Chen and Dong, et al.
‚Ä¢Equality of Opportunity :Given the prediction model, the likelihood of being in the positive class is the
same for each protected group. Formally, the prediction ÀÜùëåsatisfies the equality of opportunity if ùëÉ(ÀÜùëå=1|ùê¥=
0,ùëå=1)=ùëÉ(ÀÜùëå=1|ùê¥=1,ùëå=1).
"
73,"‚Ä¢Counterfactual fairness a causal model, the prediction counterfactually fair if
under any context, for all for any value ùëé‚Ä≤attainable by .
Besides these general definitions. user attributes, the concept of fairness has been generalized to multiple
dimensions in recommender systems , spanning from fairness-aware ranking, fairness in terms of
user psychological characteristics , supplier fairness in two-sided marketplace platforms, provider-side
fairness to make items from different providers have a fair chance of being recommended, fairness in
group recommendation to minimize the unfairness between group members.
4.7.2 Fairness-aware Methods. In the following, we review four different ways to mitigate the unfairness issue
on recommendation.
Rebalancing. Inspired by the strategy used to tackle the class-imbalance problem, one common paradigm is to
balance the data or recommendation results. certain fairness target like demographic parity.
"
74,"Some 
adopted strategies in machine learning research are re-labeling the training data to make the proportion of
positive labels equal in the protected and unprotected groups, or re-sampling the training data to achieve
statistical parity .
This idea of rebalancing data is prevalent in fairness-aware ranking, where the fairness constraint can be
represented in various forms. Towards individual equity-to-attention fairness in rankings, previous work
propose multiple ranking functions to sort items and then achieve fairness amortized across these rankings.
Towards group fairness, is a post-processing method to achieve fair ùêæranking group
fairness criteria, in which a subset of ùêæcandidates are re-selected from a large item collection to achieve a
required proportion for a single under-represented group. Analogously, DetCons and DetConstSort formalize
the fairness as a desired distribution over sensitive attributes, and re-rank candidates to
satisfy the constraints.constrain the difference of the average recommendation performance between
two groups, and formulate the fairness-aware ranking problem as integer programming. To formulate group
fairness in terms of exposure allocation, Singh et al.propose a framework for formulating fairness constraints
on rankings, and sample rankings from an associated probabilistic algorithm to fulfill the constraints. HyPER
uses probabilistic soft logic rules to balance the ratings for both users in protected and unprotected groups,
where fairness constraints are encoded as a set of rules. More recently, when organizing user-item interactions
in the form of graph, some work study potential unfairness issue inherent within graph embedding.
Among them, Fairwalk treats the group information . sensitive attributes as a prior distribution, and
then performs node2vec based on the prior to sample random walks and generate debiased embeddings, which
are evaluated in friendship recommendation.
"
75,"Regularization. The basic idea of the regularization line is to formulate the fairness criteria as a regularizer
to guide the optimization of model. A general framework, Learned Fair Representation, is proposed in,
which generates the data representations to encode insensitive attributes of data, while simultaneously removing
any information about sensitive attributes. the protected subgroup. Formally, it is composed of three loss
components:
where ¬∑is the reconstruction loss between input data ùëãand representations with an encoder
function is the prediction error in generating prediction, such as cross entropy ¬∑is
a regularization term that measures the dependence between ùëÖand sensitive attribute ùê¥, which is defined as. Publication date: December 2020.Bias and Debias in Recommender System: A Survey and Future Directions ‚Ä¢
"
76,"fairness constraints such as demographic parity:
relies on the distance of representation ùëÖand the centroid representation the group
Using such a regularization makes the encoded representation sanitized and blind to whether or not the individual
from the protected group.
Studies on this research line have been extensively conducted by subsuming different fairness formulations
under the foregoing framework. Earlier, Kamishima et al. first claimed the importance of neutrality in recommendation, and then proposed two methods one regularization-based
matrix completion method, where the fairness regularizer is formulated as the negative mutual information between sensitive attribute ùê¥and prediction one graphical model-based method , where
the fairness regularizer accounts for the expected degree of independence between ùê¥andùëåin the graphical
model. Later, Kamishima et al. generalized these work to implicit feedback-based recommender systems.
Analogously, Yao et al.
"
77,"proposed four fairness metrics in collaborative filtering, and used similar
regularization-based optimization method to mitigate different forms of bias.
Moreover, there are some regularization-based studies working on more specific scenarios. For example,
Abdollahpouri et al.focused on controlling popularity bias in learning-to-rank recommendation, and pro-
posed a regularizer that measures the lack of fairness for the short-head and medium-tail item sets in a given
recommendation list to improve fairness during model training. Xiao et al. worked on fairness-aware
group recommendation, and designed a multi-objective optimization model to minimize the utility gap between
group members. Burke et al.proposed a regularization-based matrix completion method to reweigh differ-
ent neighbors, in order to balance the fairness between protected and unprotected neighbors in collaborative
recommendation. Zhu et al.presented a fairness-aware tensor-based recommendation approach, which
uses sensitive latent factor matrix to isolate sensitive features and then uses a regularizer to extract sensitive
information which taints other factors. More recently, going beyond the pointwise fairness metrics in ranking,
Beutel et al. considered pairwise fairness of user preference between clicked and unclicked items, and offered
a new regularizer to encourage improving this metric.
"
78,"Besides in optimization objective, regularization also has been added in the ranking policy to address the
unfairness issue. considers the problem in dynamic ranking system, where the ranking function dynamically
evolves based on the feedback that users provide, and present a new sorting criterion FairCo as follows:
where the error term errmeasures the fairness violation has been introduced. The intuition behind FairCo is
that the error term pushes the items from the underexposed groups upwards in the ranking lists. Fairness-aware
constraints have also been introduced in models. Efficient policy-gradient
algorithms have been developed for model optimization.
Adversarial Learning. Similar with the idea of, the line of adversarial learning aims to get
fairness as a side-effect of fair representation. The basic idea is to play a min-max game between the prediction
model and an adversary model, where the adversary tries to predict the sensitive attributes from the data
representations, so minimizing the performance of the adversary is to remove the information pertinent to
the sensitive attributes in the representation. Towards this goal, a general framework, Adversarial Learned Fair. Publication date: Decembe.Chen and Dong, et al.
Recommendation with attribute protection.
"
79,"The figure was reproduced from the authors‚Äô
permission.
Representation , is proposed in which is formulated as follows:

where is the reconstruction loss to quantify the information retained in the representations ùëÖabout the
dataùëãby the ability of an encoder or decoder network; is to predict a predictor network;
ùúÉencompasses the parameters of the encoder/decoder and predictor networks; and is to quantify the
independence between the representation ùëÖand the sensitive attributes ùê¥via an adversary network.
Assumingùê¥is binary is formulated as for binary adversary network
which satisfies the fairness constraint of demographic parity. Maximizing is to optimize the adversary‚Äôs
parameters , while minimizing is to optimize the representation parameters .
Only recently have researchers considered this line in the field of recommendation. For example, et
al. leveraged adversarial learning to enhance the score distribution similarity between different groups.
Bose et al. and Wu et al.
"
80,"extended the ALFR framework by enforcing compositional fairness constraints
on graph embeddings for multiple sensitive attributes, which are evaluated in the scenarios of item or friendship
recommendation. Wherein, instead of fair. single sensitive attribute, it makes the graph embeddings be in-
variant. different combinations of sensitive attributes by employing a compositional encoder in the adversary
network. Building upon the ALFR framework, et al. proposed a framework termed recommendation with
attribute protection to recommend items based on user preference, while simultaneously defensing against
private-attribute inference attacks. In particular, the prediction and adversarial networks are instantiated as the
sensitive attribute inference attacker and the Bayesian personalized recommender, respectively. Analogically, Li
et al.targeted at personalized counterfactual fairness with leveraging adversarial learning to isolated the
personalized sensitive attributes.
Causal Modeling. Inspired by the success of causal modeling , studying fairness from the causal has attracted increasing attentions. In general, fairness is formulated as the causal
effect of the sensitive attribute, which is evaluated by applying counterfactual interventions over a causal graph.
For example, et al.
"
81,"focused on fairness-aware ranking, and argued that the fairness constraints based. Publication date: December 2020.Bias and Debias in Recommender System: A Survey and Future Directions
on statistical parity hardly measure the discriminatory effect. Hence, they built a causal graph that consists of the
discrete profile attributes and the continuous score, and proposed a path-specific effect technique to detect and
remove both direct and indirect rank bias. Kusner et al. introduced the notion of counterfactual fairness,
which is derived from Pearl‚Äôs causal model. It considers the causal effect by evaluating the counterfactual
intervention ‚Äî more formally, for a particular individual, whether its prediction in the real world is identical to
that in the counterfactual world where the individual‚Äôs sensitive attributes had been different.
Others. There are some other strategies on unfairness. For example. proposed to add an autoencoder
layer when learning user and item representation. This treatment can enforce that the specific unique properties
of all users and items are sufficiently preserved in the representation, mitigating the bias towards mainstream
users. Islam et al. first computed a group-specific bias direction, and then debiased each user representation
by subtracting its component in that direction. Ge et al.studied on the problem of long-term fairness and
proposed a fairness-constrained reinforcement learning algorithm to adapt dynamic fairness requirement.
4.8 Methods for Mitigating Loop Effect
Practise recommender systems usually create a pernicious feedback loop, which will create bias and further
intensify bias over time. To deal with this problem, besides the aforementioned strategies on a specific bias, a
surge of methods have been proposed recently to reduce the iterated bias that occurs during the successive
interaction between users and recommender system.
Uniform data. Leveraging uniform data is the most straightforward way to address the problem. To collect
uniform data, this kind of methods intervene in the system by using a random logging policy instead of a normal
recommendation policy. That is, for each user, they do not use the recommendation model for item delivery,
but instead randomly select some items and rank them with a uniform distribution. The uniform data
often provide gold-standard unbiased information because it breaks the feedback loop and is not affected by
various biases. However, the uniform policy would inevitably hurt users‚Äô experience and the revenue of the
platform, thus it is usually restricted to a small percentage of online traffic. Therefore, how to correct the bias
with a small uniform data is a key research question. Yuan et al. learn a imputation model from the uniform
data and apply the model to impute the labels of all displayed or non-displayed items. Rosenfeld et al. and
Bonner et al. employ two recommendation models for the biased data and uniform data, and further use a
regularization term to transfer the knowledge between the models; Liu et al.leverage knowledge distillation
to extract information from uniform data to learn a unbiased recommendation model. leverage
influence function to reweight training instances so that it has less loss in an unbiased validation set. Chen et
al.proposed to learn the optimal debiasing configures from the unbiased data.
Reinforcement learning. Collecting uniform data with a random policy is not a satisfactory strategy as it
hurts recommendation performance. Smarter recommendation strategy or policy needs to be explored. There
exists an exploration-exploitation dilemma in recommender system, where the exploitation is to recommend
items that are predicted to best match users‚Äô preference, while the exploration is to recommend items randomly
to collect more unbiased user feedback to better capture user preference. To deal with this problem, a large
number of work explores interactive recommendation by building a reinforcement learning  agent. Figure 9
illustrates the system-user interactions with a RL agent. Different from traditional recommendation methods,
RL considers the information seeking tasks as sequential interactions between an RL agent (system) and users
(environment). During the interaction, the agent can continuously update its strategies ùúãaccording to users‚Äô
history information or feedback and generates a list of items that best match users‚Äô
preferences or explore users‚Äô preference for long term reward. Then, the users will give the feedback on the recommendation lists to update the agent. Therefore, RL could balance the
competition between the exploitation and exploration and maximize each user‚Äôs long term satisfaction with. Publication date: December.
Fig. 9. The system-user interactions with a RL aggent. The figure is plotted referring to  with permission.
the system . Some recent work balance exploitation and exploration in bandit setting
withùúÄ-greedy, Boltzmann Exploration or Upper Confidence Bounds (UCB). Some work estimates action-value
reward function network using the Bellman equation and finds the best strategy with the largest
function value. Also, the actor network has been adopted recently to learn the best policy by
maximizing the long term reward.
A challenge of recommender is how to evaluate a policy. It is best to deploy it online, in the form
of an A/B test, which however is expensive and time-consuming in terms of engineering and logistic overhead
and also may harm the user experience when the policy is not mature . Off-policy evaluation is an alternative
strategy that uses historical interaction data to estimate the performance of a new policy. However, off-policy
evaluation will suffer from bias as the data are collected by an existing biased logging policy instead of uniform
policy. To correct the data bias, Chen et al.proposes to weight the policy gradient with the inverse of the
probability of historical policy. Inspired by, some work further explore off-policy evaluation
for non-stationary recommendation environments or slate recommendation. However, as claimed by Jeunen et
al. existing off-policy learning methods usually fail due to stochastic and sparse rewards. Therefore, they
further propose to leverage supervised signal with IPS strategy to better evaluate a policy. Nevertheless, off-policy
evaluation is still a challenging task especially when the historical policy is not provided, which deserves for
further exploration.
Others. There are some other strategies to mitigate the loop effect. Sun et al. leverage blind spot term
to let items be close to each other in the latent space. Sinha et al.provide an algorithm for deconvolving
feedback loops to recover users‚Äô truth rating values.
This section discusses open issues and point out some future directions.
5.1 Evaluation of Propensity Scores
As mentioned before, Inverse Propensity Score is a conventional strategy to debias. However, the effectiveness
and unbiasedness of an IPS strategy are guaranteed only when the propensity scores are properly specified. How
to obtain proper propensity scores remains an important research question. Existing methods usually assume
the ideal propensities are given. Although the evaluation of propensity scores in some simple scenarios, for
position bias, have been explored, evaluating propensity scores in more complex scenarios, such as for selection
bias or exposure bias, is still an open problem and deserves further exploration. Publication date: December 2020.Bias and Debias in Recommender System: A Survey and Future Directions ‚Ä¢
"
82,"5.2 General Debiasing Framework
From former studies, we can find that existing methods are usually designed for just addressing one or two specific
biases. However, in the real world, various biases usually occur simultaneously. For example, users usually rate
the items that they like and their rating values are influenced by the public opinions, where conformity bias and
selection bias are mixed in the collected data. Besides, the distribution of rated user-item pairs is usually inclined
to popular items or specific users groups, making the recommendation results easily suffer from popularity bias
and unfairness. It is imperative that recommender systems require a general debiasing framework to handle the
mixture of biases. It is a promising but largely under-explored area where more studies are expected. Although
challenging, the simple case ‚Äî the mixture of just two or three biases ‚Äî is worth to be explored first.
IPS or its variants, which have been successfully applied for various biases, are a promising solution for this
problem. It will be interesting and valuable to explore a novel IPS-based framework, which summarizes the
applications of IPS on different kinds of biases and provides a general propensity score learning algorithm.
"
83,"5.3 Better Evaluation
How to evaluate a recommender system in an unbiased manner? It is an essential question for both researchers
and practitioners in this area. Existing methods either require accurate propensity scores or rely on a considerable
amount of unbiased data. However, the accuracy of the former can not be guaranteed, while the latter hurts user
experience and is usually constrained on a very small percentage of online traffic. Uniform data provides gold-
standard unbiased information but its small scale makes it insufficient to thoroughly evaluate a recommendation
model due to high variance. Exploring new evaluators using large-scale biased data and small-size unbiased data
will be an interesting direction. More theoretical studies are expected, analyzing the expectation, bounds and
confidences of the proposed evaluator.
Due to popularity bias and unfairness, the evaluation exhibits more difficulties. Different work usually adopts
different evaluation criteria of popularity bias or unfairness. This creates an inconsistent reporting of scores, with
each author reporting their own assortment of results. The performance or comparisons of existing methods can
not be well understood. As such, we believe that a suite of benchmark datasets and standard evaluations metrics
should be proposed.
"
84,"5.4 Knowlege-enhanced Debiasing
It is natural that exploiting the abundant auxiliary information would improve the efficacy of debiasing. Recent
years have seen some examples that leverage attributes of users or items to correct biases in recommendation.
An interesting direction is how to better exploit this auxiliary information as the attributes are not isolated
but connected with each other forming a knowledge graph. The knowledge graph captures much more rich
information, which could be useful to understand the data bias. For example, given a user ùë¢1watches movies ùëñ1
andùëñ2, both of which are directed by the same person ùëù1and of the same genre ùëù2. From the knowledge graph,
we can deduce that the ùë¢1are highly likely to have known the movies that connect with entities ùëñ1,ùëñ2,ùëù1orùëù2.
This exposure information is important for exposure bias correction. Another advantage of knowledge graph
is its generality. All data, data sources, and databases of every type can be represented and operationalized by
the knowledge graph. Knowledge graph would be a powerful tool for developing a feature-enhanced general
debiasing framework.
"
85,"5.5 Explanation and Reasoning with Causal Graph
Cause graph is an effective mathematical tool for elucidating potentially causal relationships from data, deriving
causal relationships from combinations of knowledge and data, predicting the effects of actions, and evaluating
explanations for observed events and scenarios. As such, it is highly promising for the debiasing tasks in. Publication date: December ‚Ä¢Chen and Dong.
recommendation. On the one hand, the key of debaising is to reason the occurrence, cause, and effect over
recommendation models or data. Most biases can be understood with mild cause assumptions and additional
confounding factors in the causal graph. The effect of bias also can be inferred through the casual paths in
the graph. On the other hand, recommendation is usually considered as an intervention analogous to treating
a patient with a specific drug, where counterfactual reasoning needs to be conducted. What happens if the
recommended items are exposed to the users? Causal graph provides potentials to answer this question. The
formulated unbiased recommendation criteria can be derived with causal graph.
Nowadays, making explainable recommendations is increasingly important as it helps to improve the trans-
parency, persuasiveness, effectiveness, trustworthiness, and satisfaction of a RS. Explainable recommendation
and debiasing are highly related in the sense that they both address the problem of why: they both need to
answer why certain items are recommended by the algorithm. When causal graph is promising to address the
bias problem in a RS, it can also provide opportunities to give explanation from the strong causal paths in the
graph.
To this end, the next step would to be to design better and suitable causal graph, which is capable of reasoning,
debiasing, and explanation. We believe causal model will bring the recommendation research into a new frontier.
"
86,"5.6 Dynamic Bias
In real world, biases are usually dynamic rather than static. For example, the fashion of clothes changes frequently;
users experience many new items and may get new friends every days; the recommendation system will update
its recommendation strategy periodically. All in all, factors or biases often evolve with the time going by. It
will be interesting and valuable to explore how bias evolves and analyze how the dynamic bias affects a RS.
5.7 Double-edged Sword of Bias
Bias is not always harmful. For example, popularity bias has been validated as a double-edged sword.
Popularity bias not only results from conformity but also item quality. Appropriately leveraging popularity bias
in recommendation may improve the performance. It will be interesting and valuable to explore the double-edged
nature of other biases, fostering their benign effects while circumventing their harmful.
"
87,"5.8 Fairness-Accuracy Trade-off
The trade-off between accuracy and fairness is of importance in recommendation scenarios, where equally
treating different groups . sensitive attributes has been shown to sacrifice the recommendation performance.
Hence, it inspires us to identify specific unfairness issues; and define the fairness criteria carefully to
cover a wide range of use cases; and design some controllable methods, where the trade-off between fairness
and accuracy can be controlled. Moreover, existing methods largely assume that the sensitive attributes of
users are provided as part of the input. Such assumptions might not hold in certain real-
world scenarios ‚Äî for example, in collaborative filtering, user profiles including sensitive attributes like age and
gender cause different patterns of their behaviors; however, such profiles are unobserved but implicitly affect
the recommendation performance. A research direction is to understand the dimensions of causality and design
fairness-aware collaborative filtering algorithms in case sensitive attributes are not readily available.
"
88,"6 CONCLUSIONS
In this article, with reviewing more than 180 papers, we systematically summarize the seven kinds of biases
in recommendation, along with providing their definitions and characteristics. We further devise a taxonomy
to organize and position existing debiasing approaches, with discussing their strengths and weaknesses. We
list some open problems and research topics worth to be further explored. We hope this survey can benefit
"
